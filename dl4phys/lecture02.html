
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lecture 2 - Gradient descent &#8212; Deep Learning for Particle Physicists</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 3 - Neural network deep dive" href="lecture03.html" />
    <link rel="prev" title="Lecture 1 - Jet tagging with neural networks" href="lecture01.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for Particle Physicists</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Deep Learning for Particle Physicists
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture01.html">
   Lecture 1 - Jet tagging with neural networks
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 2 - Gradient descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture03.html">
   Lecture 3 - Neural network deep dive
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture04.html">
   Lecture 4 - Jet images and transfer learning with CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture05.html">
   Lecture 5 - Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture06.html">
   Lecture 6 - Generating hep-ph titles with Transformers
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/lewtun/dl4phys/blob/main/lecture02.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/lewtun/dl4phys"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/lecture02.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-objectives">
   Learning objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-gradient-descent">
   Stochastic gradient descent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculating-gradients">
     Calculating gradients
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-toy-example">
   A toy example
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-1-initialize-the-parameters">
     Step 1: Initialize the parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-2-calculate-the-predictions">
     Step 2: Calculate the predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-3-calculate-the-loss">
     Step 3: Calculate the loss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-4-calculate-the-gradients">
     Step 4: Calculate the gradients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-5-step-the-weights">
     Step 5: Step the weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-6-repeat-the-process">
     Step 6: Repeat the process
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-7-stop">
     Step 7: Stop
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 2 - Gradient descent</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-objectives">
   Learning objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-gradient-descent">
   Stochastic gradient descent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculating-gradients">
     Calculating gradients
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-toy-example">
   A toy example
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-1-initialize-the-parameters">
     Step 1: Initialize the parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-2-calculate-the-predictions">
     Step 2: Calculate the predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-3-calculate-the-loss">
     Step 3: Calculate the loss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-4-calculate-the-gradients">
     Step 4: Calculate the gradients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-5-step-the-weights">
     Step 5: Step the weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-6-repeat-the-process">
     Step 6: Repeat the process
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-7-stop">
     Step 7: Stop
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-2-gradient-descent">
<h1>Lecture 2 - Gradient descent<a class="headerlink" href="#lecture-2-gradient-descent" title="Permalink to this headline">#</a></h1>
<blockquote>
<div><p>A look at optimising functions with gradient descent</p>
</div></blockquote>
<section id="learning-objectives">
<h2>Learning objectives<a class="headerlink" href="#learning-objectives" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Understand what stochastic gradient descent is and how to minimise functions with it in PyTorch</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>This notebook is a condensed version of Chapter 4 of <a class="reference external" href="https://github.com/fastai/fastbook"><em>Deep Learning for Coders with fastai &amp; PyTorch</em></a> by Jeremy Howard and Sylvain Gugger. We recommend reading that chapter in detail to get the full context.</p></li>
</ul>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uncomment and run this cell if using Colab, Kaggle etc</span>
<span class="c1"># %pip install fastai==2.6.0</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">fastai.tabular.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="stochastic-gradient-descent">
<h2>Stochastic gradient descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">#</a></h2>
<p>What does it actually mean to <em>train</em> a model? In deep learning, this process is called <em>stochastic gradient descent</em> (SGD). As described in the fastai book, this process involves 7 main steps:</p>
<ol class="simple">
<li><p>Initialize the parameters of the neural network</p></li>
<li><p>For each example in the dataset, use the parameters to make a predicition (e.g. is this jet produced by a top-quark or QCD background?)</p></li>
<li><p>Use these predictions to calculate the model performance via the <em>loss</em></p></li>
<li><p>Calculate the <em>gradients</em></p></li>
<li><p>Update all the parameters by taking a <em>step</em> in the direction that minimises the loss</p></li>
<li><p>Repeat from step 2</p></li>
<li><p>Stop the training process once the model is good enough</p></li>
</ol>
<p>In this lecture, we will take a deep dive into how these steps work in PyTorch. But before doing that, let’s take a quick look at how gradients are computed in PyTorch, as they’ll play a large role in what follows.</p>
<section id="calculating-gradients">
<h3>Calculating gradients<a class="headerlink" href="#calculating-gradients" title="Permalink to this headline">#</a></h3>
<p>To illustrate how gradients are computed in PyTorch, let’s consider a simple quadratic loss function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<p>Next, let’s create a tensor at the point we wish to calculate the gradient of <span class="math notranslate nohighlight">\(f(x)\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">xt</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(3., requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Here, the <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> argument tells PyTorch to begin recording operations on the tensor <code class="docutils literal notranslate"><span class="pre">xt</span></code>; in particular which parts of the code should be included for computing gradients. Next, let’s use this tensor to generate the output <code class="docutils literal notranslate"><span class="pre">yt</span></code> from our function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yt</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
<span class="n">yt</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(9., grad_fn=&lt;PowBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>This looks good and here PyTorch is indicating both the value of the tensor and the gradient function that will be used. So let’s now compute the gradients with the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yt</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Here, “backward” refers to <em>backpropagation</em>, which is the technique used in deep learning to compute the gradients of the loss with respect to all the parameters in the model. In this simple example we just have one parameter to consider, but in general, a neural network can have thousands to billions of parameters, and so an efficient method is needed to compute this many gradients efficiently.</p>
<p>Under the hood, PyTorch implements backprogations via an automatic differentiation engine called <em>autograd</em> that keeps a record of tensors and all executed operations (like sum, multiplication etc) as a <em>directed acyclic graph</em> (DAG). We’ll look at backpropagation in a bit more detail later, but for now the main thing to note is that the gradients are stored in the <code class="docutils literal notranslate"><span class="pre">Tensor.grad</span></code> attribute:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xt</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(6.)
</pre></div>
</div>
</div>
</div>
<p>Great, this worked since we know analytically that <span class="math notranslate nohighlight">\(f'(3) = 6\)</span>! Now let’s generalise to the case where our tensor is an array of values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">xt</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 3.,  4., 10.], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>To compute the gradients, we’ll also need to add a <code class="docutils literal notranslate"><span class="pre">sum()</span></code> operator to our function so that it returns a scalar:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>


<span class="n">yt</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
<span class="n">yt</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(125., grad_fn=&lt;SumBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Here we can see that passing an array of values and applying the sum is equivalent to computing:</p>
<div class="math notranslate nohighlight">
\[ f(x_0) = \sum_i x_i^2 \Big|_{x_0}  \]</div>
<p>Finally, let’s check the values of our gradients <span class="math notranslate nohighlight">\(f'(x_0)\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yt</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">xt</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 6.,  8., 20.])
</pre></div>
</div>
</div>
</div>
<p>Now that we know how to compute gradients, we next need to find a way to update all the weights. Let’s take a look at this with a more realistic example.</p>
</section>
</section>
<section id="a-toy-example">
<h2>A toy example<a class="headerlink" href="#a-toy-example" title="Permalink to this headline">#</a></h2>
<p>Imagine that you’re measuring some signal at fixed time steps:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">time</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>and then find the result of your measurements looks something like a parabola:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">signal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="mf">0.75</span> <span class="o">*</span> <span class="p">(</span><span class="n">time</span> <span class="o">-</span> <span class="mf">9.5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># 1 * (time - 10) ** 2 + 1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">signal</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/lecture02_33_0.png" src="_images/lecture02_33_0.png" />
</div>
</div>
<p>Using SGD, our goal will be to find a function that best fits the data. A good choice of function would be a general quadratic of the form:</p>
<div class="math notranslate nohighlight">
\[ f(t, a,b,c) = at^2 + bt + c \]</div>
<p>We can then define a function that collects the timestep <span class="math notranslate nohighlight">\(t\)</span> and the parameters <span class="math notranslate nohighlight">\(a,b,c\)</span> as separate arguments:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>
</pre></div>
</div>
</div>
</div>
<p>To define what we mean by “best” values of <span class="math notranslate nohighlight">\(a,b,c\)</span>, we’ll need to choose a loss function. For regression problems like ours, it is common to use the mean squared error, which we can define as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">preds</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have a function we with to optimise and a loss function, let’s work through the 7 steps of training a model.</p>
<section id="step-1-initialize-the-parameters">
<h3>Step 1: Initialize the parameters<a class="headerlink" href="#step-1-initialize-the-parameters" title="Permalink to this headline">#</a></h3>
<p>Since our function involves three parameters <span class="math notranslate nohighlight">\(a,b,c\)</span>, we’ll initialise random values of them using the <code class="docutils literal notranslate"><span class="pre">torch.randn()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_seed</span><span class="p">(</span><span class="mi">666</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-2.1188,  0.0635, -1.4555], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>As we did earlier, we’ve applied the <code class="docutils literal notranslate"><span class="pre">requires_grad_()</span></code> method to indicate that we wish to track the gradients of the <code class="docutils literal notranslate"><span class="pre">params</span></code> tensor. We’ve also set the seed to the number of the beast so that the results are reproducible when you run the code on your own machine 😈.</p>
</section>
<section id="step-2-calculate-the-predictions">
<h3>Step 2: Calculate the predictions<a class="headerlink" href="#step-2-calculate-the-predictions" title="Permalink to this headline">#</a></h3>
<p>The next step is compute the predictions from the “model”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="n">preds</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([20])
</pre></div>
</div>
</div>
</div>
<p>Notice that we get one prediction for each of the time step in the <code class="docutils literal notranslate"><span class="pre">time</span></code> array. We can visualise these predictions with the following helper function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">show_preds</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">signal</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">to_np</span><span class="p">(</span><span class="n">preds</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">show_preds</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/lecture02_47_0.png" src="_images/lecture02_47_0.png" />
</div>
</div>
<p>Unsuprisngly, our randomly initialised model isn’t very good - let’s see if we can improve it by adjusting the parameters!</p>
</section>
<section id="step-3-calculate-the-loss">
<h3>Step 3: Calculate the loss<a class="headerlink" href="#step-3-calculate-the-loss" title="Permalink to this headline">#</a></h3>
<p>To know how we should adjust the parameters, we need a way to indicate in which direction we should optimise them. To do so, we’ll first compute the loss:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">signal</span><span class="p">)</span>
<span class="n">loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(143901.3594, grad_fn=&lt;MeanBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>To improve this value (i.e. make it lower), we’ll need the gradients.</p>
</section>
<section id="step-4-calculate-the-gradients">
<h3>Step 4: Calculate the gradients<a class="headerlink" href="#step-4-calculate-the-gradients" title="Permalink to this headline">#</a></h3>
<p>Next we calculate the gradients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">params</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-126948.2656,   -8149.7505,    -577.4262])
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-5-step-the-weights">
<h3>Step 5: Step the weights<a class="headerlink" href="#step-5-step-the-weights" title="Permalink to this headline">#</a></h3>
<p>Next we need to update the parameters according to a learning rate. For now we’ll just ues <span class="math notranslate nohighlight">\(10^{-5}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-5</span>
<span class="n">params</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">params</span><span class="o">.</span><span class="n">data</span>
<span class="n">params</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s check if the loss has improved:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="n">mse</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">signal</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(143898.6562, grad_fn=&lt;MeanBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">show_preds</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/lecture02_61_0.png" src="_images/lecture02_61_0.png" />
</div>
</div>
<p>Okay, not much of a change after one step so let’s repeat the process a few times to see how things improve. To do so, we’ll create another helper function that combines all of the above logic:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">apply_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">prn</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">signal</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
    <span class="n">params</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">prn</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">preds</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-6-repeat-the-process">
<h3>Step 6: Repeat the process<a class="headerlink" href="#step-6-repeat-the-process" title="Permalink to this headline">#</a></h3>
<p>Now that we’ve done one step of gradient descent, it’s time to repeat a few times to see if the loss decreases:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_of_iterations</span> <span class="o">=</span> <span class="mi">1_000_000</span>

<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_of_iterations</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">200_000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">apply_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">prn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="n">show_preds</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">apply_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">prn</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "daf7b9afeecc42c2bd71196a2077c60e", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0
Loss: 143898.65625
</pre></div>
</div>
<img alt="_images/lecture02_66_2.png" src="_images/lecture02_66_2.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 200000
Loss: 252.34817504882812
</pre></div>
</div>
<img alt="_images/lecture02_66_4.png" src="_images/lecture02_66_4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 400000
Loss: 97.08727264404297
</pre></div>
</div>
<img alt="_images/lecture02_66_6.png" src="_images/lecture02_66_6.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 600000
Loss: 41.89081573486328
</pre></div>
</div>
<img alt="_images/lecture02_66_8.png" src="_images/lecture02_66_8.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 800000
Loss: 22.2598876953125
</pre></div>
</div>
<img alt="_images/lecture02_66_10.png" src="_images/lecture02_66_10.png" />
</div>
</div>
<p>Great, this seems to work! The loss is decreasing with each step, indicating that a different quadratic function is being tried with different values of the parameters <span class="math notranslate nohighlight">\(a,b,c\)</span>.</p>
</section>
<section id="step-7-stop">
<h3>Step 7: Stop<a class="headerlink" href="#step-7-stop" title="Permalink to this headline">#</a></h3>
<p>Here we stopped the process arbitrarily after 1 million steps, but in practice one would track metrics like accuracy and loss on a validation set to decide when is a good point to terminate the training.</p>
<p>All of these steps can be carried over to any deep learning problem, so next lecture we’ll see how all these steps can be applied to the jet tagging datasets from lecture 1!</p>
</section>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Generate some random linear data and use SGD to find the best parameters for a linear regression model <span class="math notranslate nohighlight">\(\hat{y} = h_\theta(\bf{x}) = \bf{\theta}\cdot {\bf x} + \theta_0 \)</span></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "dl4phys"
        },
        kernelOptions: {
            kernelName: "dl4phys",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'dl4phys'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="lecture01.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 1 - Jet tagging with neural networks</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture03.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 3 - Neural network deep dive</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Lewis Tunstall<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>