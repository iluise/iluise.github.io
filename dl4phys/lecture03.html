
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lecture 3 - Neural network deep dive &#8212; Deep Learning for Particle Physicists</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 4 - Jet images and transfer learning with CNNs" href="lecture04.html" />
    <link rel="prev" title="Lecture 2 - Gradient descent" href="lecture02.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for Particle Physicists</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Deep Learning for Particle Physicists
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture01.html">
   Lecture 1 - Jet tagging with neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture02.html">
   Lecture 2 - Gradient descent
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 3 - Neural network deep dive
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture04.html">
   Lecture 4 - Jet images and transfer learning with CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture05.html">
   Lecture 5 - Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture06.html">
   Lecture 6 - Generating hep-ph titles with Transformers
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/lewtun/dl4phys/blob/main/lecture03.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/lewtun/dl4phys"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/lecture03.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-objectives">
   Learning objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-dataset">
   The dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preparing-the-data">
     Preparing the data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression-as-a-neural-network">
   Logistic regression as a neural network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#refactoring-with-pytorch-s-functional-api">
   Refactoring with PyTorch’s functional API
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#refactoring-with-pytorch-s-nn-classes">
   Refactoring with PyTorch’s
   <code class="docutils literal notranslate">
    <span class="pre">
     nn
    </span>
   </code>
   classes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#refactoring-with-pytorch-optimizers">
     Refactoring with PyTorch optimizers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#refactoring-with-dataset-classes">
   Refactoring with
   <code class="docutils literal notranslate">
    <span class="pre">
     Dataset
    </span>
   </code>
   classes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#refactoring-with-dataloaders">
     Refactoring with DataLoaders
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#going-deeper">
   Going deeper
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#wrapping-everything-in-a-learner">
   Wrapping everything in a Learner
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 3 - Neural network deep dive</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-objectives">
   Learning objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-dataset">
   The dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preparing-the-data">
     Preparing the data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression-as-a-neural-network">
   Logistic regression as a neural network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#refactoring-with-pytorch-s-functional-api">
   Refactoring with PyTorch’s functional API
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#refactoring-with-pytorch-s-nn-classes">
   Refactoring with PyTorch’s
   <code class="docutils literal notranslate">
    <span class="pre">
     nn
    </span>
   </code>
   classes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#refactoring-with-pytorch-optimizers">
     Refactoring with PyTorch optimizers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#refactoring-with-dataset-classes">
   Refactoring with
   <code class="docutils literal notranslate">
    <span class="pre">
     Dataset
    </span>
   </code>
   classes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#refactoring-with-dataloaders">
     Refactoring with DataLoaders
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#going-deeper">
   Going deeper
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#wrapping-everything-in-a-learner">
   Wrapping everything in a Learner
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-3-neural-network-deep-dive">
<h1>Lecture 3 - Neural network deep dive<a class="headerlink" href="#lecture-3-neural-network-deep-dive" title="Permalink to this headline">#</a></h1>
<blockquote>
<div><p>A deep dive into optimising neural networks with stochastic gradient descent</p>
</div></blockquote>
<section id="learning-objectives">
<h2>Learning objectives<a class="headerlink" href="#learning-objectives" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Understand how to implement neural networks from scratch</p></li>
<li><p>Understand all the ingredients needed to define a <code class="docutils literal notranslate"><span class="pre">Learner</span></code> in fastai</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Chapter 4 of <a class="reference external" href="https://github.com/fastai/fastbook"><em>Deep Learning for Coders with fastai &amp; PyTorch</em></a> by Jeremy Howard and Sylvain Gugger.</p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#what-is-torch-nn-really">What is <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> really?</a> by Jeremy Howard.</p></li>
</ul>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uncomment and run this cell if using Colab, Kaggle etc</span>
<span class="c1"># %pip install fastai==2.6.0 datasets</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">fastai.tabular.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">datasets</span>

<span class="c1"># Suppress logs to keep things tidy</span>
<span class="n">datasets</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity_error</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-dataset">
<h2>The dataset<a class="headerlink" href="#the-dataset" title="Permalink to this headline">#</a></h2>
<p>In lecture 2, we focused on optimising simple functions with stochastic gradient descent. Let’s now tackle a real-world problem using neural networks! We’ll use the <span class="math notranslate nohighlight">\(N\)</span>-subjettiness dataset from lecture 1 that represents jets in terms of <span class="math notranslate nohighlight">\(\tau_N^{(\beta)}\)</span> variables that measure the radiation about <span class="math notranslate nohighlight">\(N\)</span> axes in the jet according to an angular exponent <span class="math notranslate nohighlight">\(\beta&gt;0\)</span>. As usual, we’ll load the dataset from the Hugging Face Hub and convert it to a Pandas <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> via the <code class="docutils literal notranslate"><span class="pre">to_pandas()</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nsubjet_ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;dl4phys/top_tagging_nsubjettiness&quot;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">nsubjet_ds</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "6fb4a1359f144a2ba5a44a814e7b586b", "version_major": 2, "version_minor": 0}
</script><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pT</th>
      <th>mass</th>
      <th>tau_1_0.5</th>
      <th>tau_1_1</th>
      <th>tau_1_2</th>
      <th>tau_2_0.5</th>
      <th>tau_2_1</th>
      <th>tau_2_2</th>
      <th>tau_3_0.5</th>
      <th>tau_3_1</th>
      <th>...</th>
      <th>tau_4_0.5</th>
      <th>tau_4_1</th>
      <th>tau_4_2</th>
      <th>tau_5_0.5</th>
      <th>tau_5_1</th>
      <th>tau_5_2</th>
      <th>tau_6_0.5</th>
      <th>tau_6_1</th>
      <th>tau_6_2</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>543.633944</td>
      <td>25.846792</td>
      <td>0.165122</td>
      <td>0.032661</td>
      <td>0.002262</td>
      <td>0.048830</td>
      <td>0.003711</td>
      <td>0.000044</td>
      <td>0.030994</td>
      <td>0.001630</td>
      <td>...</td>
      <td>0.024336</td>
      <td>0.001115</td>
      <td>0.000008</td>
      <td>0.004252</td>
      <td>0.000234</td>
      <td>7.706005e-07</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000e+00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>452.411860</td>
      <td>13.388679</td>
      <td>0.162938</td>
      <td>0.027598</td>
      <td>0.000876</td>
      <td>0.095902</td>
      <td>0.015461</td>
      <td>0.000506</td>
      <td>0.079750</td>
      <td>0.009733</td>
      <td>...</td>
      <td>0.056854</td>
      <td>0.005454</td>
      <td>0.000072</td>
      <td>0.044211</td>
      <td>0.004430</td>
      <td>6.175314e-05</td>
      <td>0.037458</td>
      <td>0.003396</td>
      <td>3.670517e-05</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>429.495258</td>
      <td>32.021091</td>
      <td>0.244436</td>
      <td>0.065901</td>
      <td>0.005557</td>
      <td>0.155202</td>
      <td>0.038807</td>
      <td>0.002762</td>
      <td>0.123285</td>
      <td>0.025339</td>
      <td>...</td>
      <td>0.078205</td>
      <td>0.012678</td>
      <td>0.000567</td>
      <td>0.052374</td>
      <td>0.005935</td>
      <td>9.395772e-05</td>
      <td>0.037572</td>
      <td>0.002932</td>
      <td>2.237277e-05</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>512.675443</td>
      <td>6.684734</td>
      <td>0.102580</td>
      <td>0.011369</td>
      <td>0.000170</td>
      <td>0.086306</td>
      <td>0.007760</td>
      <td>0.000071</td>
      <td>0.068169</td>
      <td>0.005386</td>
      <td>...</td>
      <td>0.044705</td>
      <td>0.002376</td>
      <td>0.000008</td>
      <td>0.027895</td>
      <td>0.001364</td>
      <td>4.400042e-06</td>
      <td>0.009012</td>
      <td>0.000379</td>
      <td>6.731099e-07</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>527.956859</td>
      <td>133.985415</td>
      <td>0.407009</td>
      <td>0.191839</td>
      <td>0.065169</td>
      <td>0.291460</td>
      <td>0.105479</td>
      <td>0.029753</td>
      <td>0.209341</td>
      <td>0.049187</td>
      <td>...</td>
      <td>0.143768</td>
      <td>0.033249</td>
      <td>0.003689</td>
      <td>0.135407</td>
      <td>0.029054</td>
      <td>2.593460e-03</td>
      <td>0.110805</td>
      <td>0.023179</td>
      <td>2.202088e-03</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div></div></div>
</div>
<section id="preparing-the-data">
<h3>Preparing the data<a class="headerlink" href="#preparing-the-data" title="Permalink to this headline">#</a></h3>
<p>In lecture 1, we used the <code class="docutils literal notranslate"><span class="pre">TabularDataLoaders.from_df()</span></code> method from fastai to quickly create dataloaders for the train and validation sets. In this lecture, we’ll be working with PyTorch tensors directly, so we’ll take a different approach. To get started, we’ll need to split our data into training and validation sets. We can do this easily via the <code class="docutils literal notranslate"><span class="pre">train_test_split()</span></code> function from scikit-learn:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="p">,</span> <span class="n">valid_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((908250, 21), (302750, 21))
</pre></div>
</div>
</div>
</div>
<p>This has allocated 75% of our original dataset to <code class="docutils literal notranslate"><span class="pre">train_df</span></code> and the remainder to <code class="docutils literal notranslate"><span class="pre">valid_df</span></code>. Now that we have these <code class="docutils literal notranslate"><span class="pre">DataFrames</span></code>, the next thing we’ll need are tensors for the features <span class="math notranslate nohighlight">\((p_T, m, \tau_1^{(0.5)}, \tau_1^{(1)}, \tau_1^{(2)}, \ldots )\)</span> and labels. There is, however, one potential problem: the jet <span class="math notranslate nohighlight">\(p_T\)</span> and mass have much larger scales than the <span class="math notranslate nohighlight">\(N\)</span>-subjettiness <span class="math notranslate nohighlight">\(\tau_N^{(\beta)}\)</span> features. We can see this by summarising the statistics of the training set with the <code class="docutils literal notranslate"><span class="pre">describe()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pT</th>
      <th>mass</th>
      <th>tau_1_0.5</th>
      <th>tau_1_1</th>
      <th>tau_1_2</th>
      <th>tau_2_0.5</th>
      <th>tau_2_1</th>
      <th>tau_2_2</th>
      <th>tau_3_0.5</th>
      <th>tau_3_1</th>
      <th>...</th>
      <th>tau_4_0.5</th>
      <th>tau_4_1</th>
      <th>tau_4_2</th>
      <th>tau_5_0.5</th>
      <th>tau_5_1</th>
      <th>tau_5_2</th>
      <th>tau_6_0.5</th>
      <th>tau_6_1</th>
      <th>tau_6_2</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>...</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
      <td>908250.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>487.107393</td>
      <td>88.090520</td>
      <td>0.366716</td>
      <td>0.198446</td>
      <td>0.319559</td>
      <td>0.222759</td>
      <td>0.079243</td>
      <td>0.072535</td>
      <td>0.148137</td>
      <td>0.035372</td>
      <td>...</td>
      <td>0.112024</td>
      <td>0.022150</td>
      <td>0.008670</td>
      <td>0.088400</td>
      <td>0.015329</td>
      <td>0.004875</td>
      <td>0.070679</td>
      <td>0.011019</td>
      <td>0.002914</td>
      <td>0.500366</td>
    </tr>
    <tr>
      <th>std</th>
      <td>48.568267</td>
      <td>48.393646</td>
      <td>0.186922</td>
      <td>0.339542</td>
      <td>2.003898</td>
      <td>0.110955</td>
      <td>0.125155</td>
      <td>0.674091</td>
      <td>0.072627</td>
      <td>0.051869</td>
      <td>...</td>
      <td>0.059393</td>
      <td>0.032004</td>
      <td>0.155468</td>
      <td>0.051949</td>
      <td>0.022866</td>
      <td>0.107641</td>
      <td>0.046571</td>
      <td>0.017133</td>
      <td>0.078247</td>
      <td>0.500000</td>
    </tr>
    <tr>
      <th>min</th>
      <td>225.490387</td>
      <td>-0.433573</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>452.879289</td>
      <td>39.958178</td>
      <td>0.224456</td>
      <td>0.058381</td>
      <td>0.006443</td>
      <td>0.139269</td>
      <td>0.025638</td>
      <td>0.001565</td>
      <td>0.094603</td>
      <td>0.013308</td>
      <td>...</td>
      <td>0.069037</td>
      <td>0.007949</td>
      <td>0.000188</td>
      <td>0.051012</td>
      <td>0.004936</td>
      <td>0.000079</td>
      <td>0.036142</td>
      <td>0.002977</td>
      <td>0.000033</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>485.894050</td>
      <td>99.887418</td>
      <td>0.380172</td>
      <td>0.166016</td>
      <td>0.045887</td>
      <td>0.222763</td>
      <td>0.061597</td>
      <td>0.008788</td>
      <td>0.148810</td>
      <td>0.028501</td>
      <td>...</td>
      <td>0.110220</td>
      <td>0.017609</td>
      <td>0.000787</td>
      <td>0.086045</td>
      <td>0.011755</td>
      <td>0.000387</td>
      <td>0.067797</td>
      <td>0.008028</td>
      <td>0.000193</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>520.506446</td>
      <td>126.518545</td>
      <td>0.477122</td>
      <td>0.240550</td>
      <td>0.074417</td>
      <td>0.299708</td>
      <td>0.108207</td>
      <td>0.022441</td>
      <td>0.196156</td>
      <td>0.046588</td>
      <td>...</td>
      <td>0.151137</td>
      <td>0.029990</td>
      <td>0.002006</td>
      <td>0.121905</td>
      <td>0.021089</td>
      <td>0.001103</td>
      <td>0.100437</td>
      <td>0.015359</td>
      <td>0.000635</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>647.493145</td>
      <td>299.211555</td>
      <td>2.431888</td>
      <td>6.013309</td>
      <td>37.702422</td>
      <td>2.218956</td>
      <td>5.392683</td>
      <td>33.352249</td>
      <td>1.917912</td>
      <td>4.502011</td>
      <td>...</td>
      <td>1.616280</td>
      <td>3.753716</td>
      <td>21.161948</td>
      <td>1.407356</td>
      <td>3.158352</td>
      <td>17.645603</td>
      <td>1.388879</td>
      <td>3.127371</td>
      <td>17.340970</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 21 columns</p>
</div></div></div>
</div>
<p>Here we can see that the jet <span class="math notranslate nohighlight">\(p_T\)</span> and mass have average values of around 480 and 90 GeV, while the <span class="math notranslate nohighlight">\(N\)</span>-subjettiness variables  <span class="math notranslate nohighlight">\(\tau_N^{(\beta)}\)</span> have values that are orders of magnitude smaller. As we saw in lecture 2, SGD can struggle to optimise the loss function when the feature scales are very different. To handle this, it is common to <em>normalize</em> the features. One way to do this is by rescaling all the features <span class="math notranslate nohighlight">\(x_i\)</span> to lie in the interval <span class="math notranslate nohighlight">\([0,1]\)</span>:</p>
<div class="math notranslate nohighlight">
\[ x_i' = \frac{x_i - x_{i,\mathrm{min}}}{x_{i,\mathrm{max}} - x_{i,\mathrm{min}}} \]</div>
<p>To apply this <em>minmax</em> normalization, let’s first grab the NumPy arrays of the features and labels:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Slice out all feature columns</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="c1"># Slice out the label column</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we use the <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> from scikit-learn to apply the normalization on the features array:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
<span class="c1"># Sanity check the normalization worked</span>
<span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">train_x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.0, 1.0)
</pre></div>
</div>
</div>
</div>
<p>Great, this worked! Now that our features are all nicely normalised, let’s convert these NumPy arrays to PyTorch tensors. PyTorch provides a handy <code class="docutils literal notranslate"><span class="pre">from_numpy()</span></code> method that allows us to do the conversion easily:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Cast to float32</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="c1"># Sanity check on the shapes</span>
<span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([908250, 20]), torch.Size([908250]))
</pre></div>
</div>
</div>
</div>
<p>Okay, now that we have our tensors it’s time to train a neural network!</p>
</section>
</section>
<section id="logistic-regression-as-a-neural-network">
<h2>Logistic regression as a neural network<a class="headerlink" href="#logistic-regression-as-a-neural-network" title="Permalink to this headline">#</a></h2>
<p>To warm up, let’s train the simplest type of neural network for classification tasks: logistic regression! You might be surprised to hear that logistic regression can be viewed as a neural network. However, a <em>one-layer network</em> has the same properties, so let’s look at how we can implement this in PyTorch.</p>
<p>To get started, we’ll need some weights and biases, so let’s create random tensors using a type of intialization called <a class="reference external" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"><em>Xavier initialization</em></a>. This initializes the biases to zero, while the weights <span class="math notranslate nohighlight">\(W_{ij}\)</span> are sampled from a normal distribution in the interval <span class="math notranslate nohighlight">\((-1/\sqrt{n},1/\sqrt{n})\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of features. We can implement Xavier initialization in PyTorch as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Xavier initialisation</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># Track grads after initialization</span>
<span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have the weights and biases, the next ingredient we need is an activation function. For binary classification tasks, this usually takes the form of a sigmoid function, whose generalization to <span class="math notranslate nohighlight">\(K&gt;2\)</span> classes is called the <em>softmax</em> function:</p>
<div class="math notranslate nohighlight">
\[ \sigma(\mathbf{x})_i = \frac{e^{x_i}}{\sum_{j=1}^K e^{x_j}} \qquad \mbox{for } i=1, \ldots , K\]</div>
<p>The sigmoid and the softmax functions have the effect of normalizing the output of the network to be a probability distribution. To keep things general, we’ll use the softmax in this lecture. However, implementing softmax naively presents some numerical stability challenges. Consider, for example, computing the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([inf, inf, inf])
</pre></div>
</div>
</div>
</div>
<p>Hmm, a network that outputs infinity values will will cause the learning process to crash. This is an example of <em>numerical overflow</em>. Similarly, when the inputs are large negative numbers, we end up rounding the results to zero, an example of <em>numerical underflow</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1000.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1000.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1000.0</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<p>To deal with these two problems, we can apply the <a class="reference external" href="https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/"><em>log-sum-exp</em> trick</a>:</p>
<div class="math notranslate nohighlight">
\[\log \sum_{i=1}^n e^{x_i} = a + \log \sum_{i=1}^n e^{x_i-a} \]</div>
<p>where <span class="math notranslate nohighlight">\(a = \max x_i\)</span> is a constant that forces the greatest value to be zero. Since <span class="math notranslate nohighlight">\(\log a/b = \log a - \log b\)</span>, taking the logarithm of the softmax function gives:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">())</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-1.0986, -1.0986, -1.0986])
</pre></div>
</div>
</div>
</div>
<p>Great, we now have an activation function that is numerically stable. Let’s now define our logistic regression model to take a mini-batch <code class="docutils literal notranslate"><span class="pre">xb</span></code> of inputs and output the log-softmax values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">xb</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test this model with a batch of data from our training set (also called a <em>forward pass</em>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Batch size</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="c1"># A mini-batch from x</span>
<span class="n">xb</span> <span class="o">=</span> <span class="n">train_x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">bs</span><span class="p">]</span>
<span class="c1"># Model predictions</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">preds</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([-0.5103, -0.9171], grad_fn=&lt;SelectBackward0&gt;), torch.Size([1024, 2]))
</pre></div>
</div>
</div>
</div>
<p>At this state the predictions are random, since we started with random weights. To improve these values, the next thing we need is a loss function. For classification tasks, one computes the <em>cross entropy</em>, which is the log likelihood of the softmax:</p>
<div class="math notranslate nohighlight">
\[ {\cal L} = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)}\log\hat{p}_k^{(i)} \,.\]</div>
<p>However, we’ve already taken the log of the softmax values <span class="math notranslate nohighlight">\(\hat{p}_k^{(i)}\)</span>, so instead our loss will be the <em>negative log likelihood</em>, which doesn’t include the logarithm. We can implement this easily in PyTorch as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="c1"># Mask predictions according to whether y_hat is 1 or 0</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">predictions</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">target</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>


<span class="n">loss_func</span> <span class="o">=</span> <span class="n">nll_loss</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have a loss function, let’s test we can compute the loss by comparing our mini-batch of predictions against a mini-batch of target values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yb</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">bs</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.7619, grad_fn=&lt;NegBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Again, the loss value is random, but we can minimise this function with backpropagation. Before doing that, let’s also compute the accuracy of the model so that we track progress during training:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">yb</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>


<span class="n">accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.5020)
</pre></div>
</div>
</div>
</div>
<p>Indeed, the random model has an accuracy of 50% which is what we expect before any training. To implement the training loop, we’ll take the following steps:</p>
<ol class="simple">
<li><p>Select a mini-batch of data of size <code class="docutils literal notranslate"><span class="pre">bs</span></code></p></li>
<li><p>Generate predictions from the model by computing the forward pass</p></li>
<li><p>Compute the loss</p></li>
<li><p>Compute the gradients of the loss wrt to the parameters by applying <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code></p></li>
<li><p>Update the weights and biases of the model by taking a step of gradient descent</p></li>
</ol>
<p>In code, this looks as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="c1"># Number of epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># 1. Select mini-batch</span>
        <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
        <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">train_x</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">yb</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="c1"># 2. Generate predictions</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="c1"># 3. Compute the loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="c1"># 4. Compute the gradients</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># 5. Update the weights and biases</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">weights</span> <span class="o">-=</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
            <span class="n">bias</span> <span class="o">-=</span> <span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
            <span class="c1"># Set current gradients to zero</span>
            <span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "9a0ef6e38a874b309f48d52e719a74bd", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<p>Note that here we update the weights and biases within the <code class="docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> context manager - that’s because we don’t want these updates to be recorded in the tensors in the next iteration of gradient descent. We also set the gradients to zero after the update to prevent tracking these operations with every iteration.</p>
<p>Now that we’ve trained our model, let’s compute the loss and accuracy to see if they’ve improved:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_scores</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_scores</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 0.561
Accuracy: 0.857
</pre></div>
</div>
</div>
</div>
<p>Congratulations - you’ve trained your first neural network from scratch!</p>
<p>In principle, there’s nothing wrong with using raw PyTorch tensor operations to train models, but the framework provides various functions and classes that can simplify our code and make it more robust to errors. Let’s take a look.</p>
</section>
<section id="refactoring-with-pytorch-s-functional-api">
<h2>Refactoring with PyTorch’s functional API<a class="headerlink" href="#refactoring-with-pytorch-s-functional-api" title="Permalink to this headline">#</a></h2>
<p>Instead of manually computing the log-softmax and negative log-likelihood, PyTorch provide a cross-entropy function that does all of this in one go! This function and many others live within the <code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code> module, which is usually imported into the <code class="docutils literal notranslate"><span class="pre">F</span></code> namespace. Let’s use the <code class="docutils literal notranslate"><span class="pre">F.cross_entropy</span></code> function as our loss function, which means we can remove the activation from our model’s forward pass:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">xb</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>


<span class="c1"># Sanity check we get the same scores as before</span>
<span class="n">print_scores</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 0.561
Accuracy: 0.857
</pre></div>
</div>
</div>
</div>
</section>
<section id="refactoring-with-pytorch-s-nn-classes">
<h2>Refactoring with PyTorch’s <code class="docutils literal notranslate"><span class="pre">nn</span></code> classes<a class="headerlink" href="#refactoring-with-pytorch-s-nn-classes" title="Permalink to this headline">#</a></h2>
<p>The next thing we’ll do is simplify our training loop by using the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code> classes. The first holds the weights and biases of the model, and defines the forward pass. The second, makes it simpler to keep track of the gradients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LogisticRegressor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">xb</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</pre></div>
</div>
</div>
</div>
<p>Now when we instantiate this class, we get a newly initialized model, which we can generate predictions from, compute the loss etc:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegressor</span><span class="p">()</span>
<span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.7114, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>The big advatnage of the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code> classes is that we no longer have to manually update each parameter by name and zero out the gradients. We just need to iterate over the parameters associated with <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and apply <code class="docutils literal notranslate"><span class="pre">model.zero_grad()</span></code> at the end of the updates. Let’s wrap the training loop in a <code class="docutils literal notranslate"><span class="pre">fit()</span></code> function for later use:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="c1"># 1. Select mini-batch</span>
            <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
            <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
            <span class="n">xb</span> <span class="o">=</span> <span class="n">train_x</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
            <span class="n">yb</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
            <span class="c1"># 2. Generate predictions</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
            <span class="c1"># 3. Compute the loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
            <span class="c1"># 4. Compute the gradients</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="c1"># 5. Update the weights and biases</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                    <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
                <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>


<span class="n">fit</span><span class="p">()</span>
<span class="n">print_scores</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "6eb967e973dc4aecbb05ff9657039dca", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 0.547
Accuracy: 0.859
</pre></div>
</div>
</div>
</div>
<p>We can actually simplify our model even further by using the <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> class, which defines a <em>linear layer</em> in a neural network. This class automatically initializes the weights and biases with Xavier initialization and computes <code class="docutils literal notranslate"><span class="pre">xb</span> <span class="pre">&#64;</span> <span class="pre">weights</span> <span class="pre">+</span> <span class="pre">biases</span></code> for us. Let’s use this layer and retrain our model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LogisticRegressor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegressor</span><span class="p">()</span>
<span class="n">fit</span><span class="p">()</span>
<span class="n">print_scores</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "812aa7fa951a4087bd78b5adde3dee72", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 0.530
Accuracy: 0.859
</pre></div>
</div>
</div>
</div>
<p>It works!</p>
<section id="refactoring-with-pytorch-optimizers">
<h3>Refactoring with PyTorch optimizers<a class="headerlink" href="#refactoring-with-pytorch-optimizers" title="Permalink to this headline">#</a></h3>
<p>Now let’s simplify the gradient update step of our training loop by using the <code class="docutils literal notranslate"><span class="pre">SGD</span></code> optimizer in PyTorch. This optimizer will allow us to reduce the whole logic under the <code class="docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> context manager with just two steps:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 5. Update the weights and biases</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>To do so, let’s create a simple helper function that initializes a new model and optimizer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegressor</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>


<span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>
<span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.7052, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Now that we have a model and optimizer, we can refactor our <code class="docutils literal notranslate"><span class="pre">fit()</span></code> function as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="c1"># 1. Select mini-batch</span>
            <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
            <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
            <span class="n">xb</span> <span class="o">=</span> <span class="n">train_x</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
            <span class="n">yb</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
            <span class="c1"># 2. Generate predictions</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
            <span class="c1"># 3. Compute the loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
            <span class="c1"># 4. Compute the gradients</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="c1"># 5. Update the weights and biases</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>


<span class="n">fit</span><span class="p">()</span>
<span class="n">print_scores</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "c61a7c1bd6814920b7eafd87214c1637", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 0.535
Accuracy: 0.859
</pre></div>
</div>
</div>
</div>
<p>Nice, our training loop is quite concise now, but notice that we still have to manually define the mini-batches. Let’s see how we can simplify this with the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> classes in PyTorch.</p>
</section>
</section>
<section id="refactoring-with-dataset-classes">
<h2>Refactoring with <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> classes<a class="headerlink" href="#refactoring-with-dataset-classes" title="Permalink to this headline">#</a></h2>
<p>PyTorch provides an abstract <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class that simplifies the way we access the features and labels of each mini-batch. The main requirement is that a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> should implement <code class="docutils literal notranslate"><span class="pre">__len__</span></code> and <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> functions that allow us to iterate over the data. PyTorch conveniently provides a <code class="docutils literal notranslate"><span class="pre">TensorDataset</span></code> that does this for tensors, so we can create our dataset by simply passing the tensors of features and labels:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This dataset has a length:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">train_ds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>908250
</pre></div>
</div>
</div>
</div>
<p>and we can index into it like a Python list:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([6.1467e-01, 3.4553e-01, 2.0827e-01, 6.1930e-02, 2.9217e-02, 1.1426e-01,
         3.9722e-02, 3.0127e-02, 1.1553e-01, 4.4638e-02, 3.7694e-02, 1.1224e-01,
         5.0610e-02, 4.7183e-02, 7.6042e-02, 4.5295e-03, 2.0119e-05, 5.5926e-02,
         2.8786e-03, 8.6186e-06]),
 tensor(1))
</pre></div>
</div>
</div>
</div>
<p>Note that each element returns a <em>tuple</em> of the feature and corresponding label. This means we can replace the mini-batch step to a single line of code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">bs</span> <span class="p">:</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span> <span class="o">+</span> <span class="n">bs</span><span class="p">]</span>
</pre></div>
</div>
<p>Let’s refactor our <code class="docutils literal notranslate"><span class="pre">fit()</span></code> function to use the <code class="docutils literal notranslate"><span class="pre">train_ds</span></code> object now:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">fit</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="c1"># 1. Select mini-batch</span>
            <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">bs</span> <span class="p">:</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span> <span class="o">+</span> <span class="n">bs</span><span class="p">]</span>
            <span class="c1"># 2. Generate predictions</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
            <span class="c1"># 3. Compute the loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
            <span class="c1"># 4. Compute the gradients</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="c1"># 5. Update the weights and biases</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>


<span class="n">fit</span><span class="p">()</span>
<span class="n">print_scores</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "d68cf2e2836b4786abddfb287c8d0c97", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 0.545
Accuracy: 0.856
</pre></div>
</div>
</div>
</div>
<section id="refactoring-with-dataloaders">
<h3>Refactoring with DataLoaders<a class="headerlink" href="#refactoring-with-dataloaders" title="Permalink to this headline">#</a></h3>
<p>We can actually simplify our training loop even further by using a PyTorch <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> class to manage the way we grab mini-batches. A <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> receives a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and returns a generator we can iterate over:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dl</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[tensor([[3.1934e-01, 3.0916e-01, 1.7249e-01,  ..., 5.0833e-02, 2.5030e-03,
          6.6223e-06],
         [5.9345e-01, 1.3606e-01, 1.0415e-01,  ..., 6.3039e-02, 3.7498e-03,
          2.1251e-05],
         [7.4757e-01, 3.8743e-01, 1.3758e-01,  ..., 2.7057e-02, 1.9911e-03,
          2.2671e-05],
         ...,
         [5.9834e-01, 3.3535e-01, 1.8187e-01,  ..., 2.6380e-02, 1.0312e-03,
          1.8795e-06],
         [6.3238e-01, 4.1819e-01, 1.7070e-01,  ..., 6.9010e-02, 5.0007e-03,
          1.0914e-04],
         [7.1288e-01, 9.3378e-02, 6.7393e-02,  ..., 2.5031e-02, 7.3758e-04,
          2.1064e-06]]),
 tensor([1, 0, 1,  ..., 1, 1, 0])]
</pre></div>
</div>
</div>
</div>
<p>We can then simply iterate over the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> to get our mini-batches for the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">fit</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">):</span>
        <span class="c1"># 1. Select mini-batch</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="c1"># 2. Generate predictions</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
            <span class="c1"># 3. Compute the loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
            <span class="c1"># 4. Compute the gradients</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="c1"># 5. Update the weights and biases</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>


<span class="n">fit</span><span class="p">()</span>
<span class="n">print_scores</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "c992152104e845cdaf5b8ece7ed7e1f6", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 0.537
Accuracy: 0.858
</pre></div>
</div>
</div>
</div>
<p>Great, we now have a rather simple training loop that works with any type of model! Let’s now use a full-blown neural network with several hidden layers!</p>
</section>
</section>
<section id="going-deeper">
<h2>Going deeper<a class="headerlink" href="#going-deeper" title="Permalink to this headline">#</a></h2>
<p>Our logistic regression model is actually pretty good, but in many applications you’ll want a <em>deep</em> neural network to get better performance. To create neural networks, PyTorch provides a <code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> class that allows you to stack layers one after another. Let’s implement the same architecture defined in the top tagging review from the top tagging review:</p>
<blockquote>
<div><p>The network consists of four fully connected hidden
layers, the first two with 200 nodes and a dropout regularization of 0.2, and the last two
with 50 nodes and a dropout regularization of 0.1. The output layer consists of two nodes.
We use a ReLu activation function throughout and minimize the cross-entropy using Adam
optimization</p>
</div></blockquote>
<p>We briefly encountered dropout in the last lecture, so let’s quckly explain how it works. Dropout is a <em>regularization technique</em> (not the type of regularization you’re familiar from QFT though!), that is designed to prevent the model from overfitting. The basic idea is to randomly change some of the activations in the network to zero during training time. An animation of the process is shown below, which shows how this process introduces some noise into the process and produces a more robust network:</p>
<p><img alt="" src="_images/dropout.gif" /></p>
<p>Now we can’t just zero out activations naively because this will screw up the scales across each layer. Insted we apply dropout with probability <code class="docutils literal notranslate"><span class="pre">p</span></code> and then rescale all activations by <code class="docutils literal notranslate"><span class="pre">1-p</span></code> to keep the scales well behaved.</p>
<p>The resulting model from the review article thus looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And just like before, we can define the optimizer. In this case we’ll use a special optimizer called Adam, which combines SGD with some other techniques to speed up training. You can find the details of Adam in Chapter 16 of the fastai book, but for now, we’ll just instantiate it from PyTorch:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">fit</span><span class="p">()</span>
<span class="n">print_scores</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "28c66060a17340c6a90305f8beefe93b", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 0.263
Accuracy: 0.890
</pre></div>
</div>
</div>
</div>
<p>Not bad, we’ve got a decent boost from using a deeper model and better optimizer!</p>
</section>
<section id="wrapping-everything-in-a-learner">
<h2>Wrapping everything in a Learner<a class="headerlink" href="#wrapping-everything-in-a-learner" title="Permalink to this headline">#</a></h2>
<p>To wrap things up, let’s show how we can feed all these building blocks into a fastai <code class="docutils literal notranslate"><span class="pre">Learner</span></code> that takes care of the training loop for us. First we’ll need to create a validation set for evaluation, so let’s do that using the same techniques we did for the training set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Slice out all feature columns and cast to float32</span>
<span class="n">valid_x</span> <span class="o">=</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">valid_x</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">valid_x</span><span class="p">)</span>
<span class="n">valid_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">valid_x</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="c1"># Slice out the label column</span>
<span class="n">valid_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">valid_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="c1"># Create dataset and dataloader for validation set</span>
<span class="n">valid_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">valid_x</span><span class="p">,</span> <span class="n">valid_y</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have dataloaders, recall that fastai wraps them in a single <code class="docutils literal notranslate"><span class="pre">DataLoaders</span></code> object:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">DataLoaders</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The final step is to define the model and optimizer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">opt_func</span> <span class="o">=</span> <span class="n">Adam</span>
</pre></div>
</div>
</div>
</div>
<p>and wrap everything in a <code class="docutils literal notranslate"><span class="pre">Learner</span></code> and train for 3 epochs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">opt_func</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.251400</td>
      <td>0.311221</td>
      <td>0.834794</td>
      <td>00:13</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.243241</td>
      <td>0.369533</td>
      <td>0.796215</td>
      <td>00:13</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.242842</td>
      <td>0.313126</td>
      <td>0.863372</td>
      <td>00:13</td>
    </tr>
  </tbody>
</table></div></div>
</div>
<p>Well, this was quite a deep dive into traiing neural networks from scratch and ending with with all the components that go into a fastai <code class="docutils literal notranslate"><span class="pre">Learner</span></code>!</p>
<p>Next week, we’ll move away from tabular data and take a look a class of neural networks for images that are based on convolutions 👀.</p>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Instead of using <code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> to create our neural network, try implementing this as a subclass of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and training the resulting model.</p></li>
<li><p>Using the validation dataset and dataloader, try computing the validation loss and accuracy within the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> function.</p></li>
<li><p>Read the <a class="reference external" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"><em>Xavier initialization</em> paper</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="lecture02.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 2 - Gradient descent</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture04.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 4 - Jet images and transfer learning with CNNs</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Lewis Tunstall<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>